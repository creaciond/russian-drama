{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Структурные особенности ремарок // Structural peculiarities of stage directions\n",
    "По мотивам [Сперантов 1998](https://rvb.ru/philologica/05/05sperantov.htm): попытаемся интерпретировать пьесы из RusDraCor с помощью метрик \"классичности\", предложенных в статье. \n",
    "\n",
    "***\n",
    "\n",
    "In English: see below in each section.\n",
    "\n",
    "This notebook interprets the article written by Sperantov in 1998 (see it [here](https://rvb.ru/philologica/05/05sperantov.htm)) and applies the same metrics to RusDraCor.\n",
    "\n",
    "## Коэффициент плотности // Density\n",
    "Отвечает за частоту появления ремарок.\n",
    "\n",
    "$$P = \\frac{n}{N}\\cdot100,\\ \\ \\ \\ \\ \\ \\ (1)$$\n",
    "где $n$ — число ремарок, $N$ — число стихотворных строк.\n",
    "\n",
    "_Адаптация:_ число стихотворных строк => число реплик (`<sp>`)\n",
    "\n",
    "***\n",
    "\n",
    "Shows how frequent the directions are.\n",
    "\n",
    "$(1)$: $n$ — overall stage directions count, $N$ — overall count of verse lines.\n",
    "\n",
    "_Adaptation:_ verse lines => `<sp>` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from pymystem3 import Mystem\n",
    "from statistics import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tei_ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "corpus_path = \"..\" + os.sep + \"RusDraCor\"\n",
    "directions_path = \"..\" + os.sep + \"directions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plays = [file[:-4] for file in os.listdir(corpus_path) if file != \".DS_Store\"]\n",
    "data_total = {\"file\": plays}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечём дату, чтобы можно было посмотреть на распределение на типы по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_date(date_print, date_premiere, date_written):\n",
    "    if date_print and date_premiere:\n",
    "        date_definite = min(date_print, date_premiere)\n",
    "    elif date_premiere:\n",
    "        date_definite = date_premiere\n",
    "    else:\n",
    "        date_definite = date_print\n",
    "    \n",
    "    if date_written and date_definite:\n",
    "        if date_definite - date_written > 10:\n",
    "            date_definite = date_written\n",
    "        elif date_written and not date_definite:\n",
    "            date_definite = date_written\n",
    "    return date_definite\n",
    "\n",
    "def get_play_definite_year(play_path):\n",
    "    root = etree.parse(play_path)\n",
    "    try:\n",
    "        written_str = root.find(\".//tei:sourceDesc/tei:bibl/tei:bibl/tei:date[@type=\\\"written\\\"]\", \n",
    "                                tei_ns).attrib[\"when\"]\n",
    "        year_written = int(written_str)\n",
    "    except:\n",
    "        year_written = None\n",
    "    try:\n",
    "        published_str = root.find(\".//tei:sourceDesc/tei:bibl/tei:bibl/tei:date[@type=\\\"print\\\"]\", \n",
    "                                  tei_ns).attrib[\"when\"]\n",
    "        year_published = int(published_str)\n",
    "    except:\n",
    "        year_published = None\n",
    "    try:\n",
    "        premiere_str = root.find(\".//tei:sourceDesc/tei:bibl/tei:bibl/tei:date[@type=\\\"premiere\\\"]\", \n",
    "                                 tei_ns).attrib[\"when\"]\n",
    "        year_premiere = int(premiere_str)\n",
    "    except:\n",
    "        year_premiere = None\n",
    "    year_definite = single_date(year_published, year_premiere, year_written)\n",
    "    return year_definite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_year = []\n",
    "for play in plays:\n",
    "    play_file = corpus_path + os.sep + play + \".xml\"\n",
    "    year = get_play_definite_year(play_file)\n",
    "    data_year.append(year)\n",
    "\n",
    "data_total[\"year\"] = data_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: regexp => xpath\n",
    "# xpath_sp = \"//sp\"\n",
    "\n",
    "data_sp = []\n",
    "reg_sp = re.compile(\"<sp.*?>\")\n",
    "\n",
    "for play in plays:\n",
    "    play_file = corpus_path + os.sep + play + \".xml\"\n",
    "    # root = etree.parse(play_file)\n",
    "    # print(root.tostring())\n",
    "    # sp_all = root.findall(xpath_sp, tei_ns)\n",
    "    with open(play_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        play_text = f.read()\n",
    "        sp_all = re.findall(reg_sp, play_text)\n",
    "        print(\"Play: {}, <sp>: {}\".format(play, len(sp_all)))\n",
    "        data_sp.append(len(sp_all))\n",
    "        data_total[\"sp\"] = data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stage = []\n",
    "for play in plays:\n",
    "    directions_file = directions_path + os.sep + play + \".txt\"\n",
    "    with open(directions_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        directions_list = f.readlines()\n",
    "        total_dirs = len(directions_list)\n",
    "        data_stage.append(total_dirs)\n",
    "        print(\"Play: {}, <stage>: {}\".format(play, total_dirs))\n",
    "data_total[\"stage\"] = data_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_p = []\n",
    "for i, play in enumerate(plays):\n",
    "    sp = data_sp[i]\n",
    "    stage = data_stage[i]\n",
    "    if sp != 0:\n",
    "        p = stage/sp*100\n",
    "        print(\"Play: {}, p={:.4f}\".format(play, p))\n",
    "    else:\n",
    "        p = 0\n",
    "        print(\"Unable to count p for play: {}\".format(play))\n",
    "    data_p.append(p)\n",
    "data_total[\"p\"] = data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame.from_dict(data_total)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_metric(metric_list):\n",
    "    metric_min = min(metric_list)\n",
    "    metric_max = max(metric_list)\n",
    "    metrics_norm = []\n",
    "    for metric in metric_list:\n",
    "        norm = (metric-metric_min)/(metric_max-metric_min)\n",
    "        metrics_norm.append(norm)\n",
    "    return metrics_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total[\"p_norm\"] = normalize_metric(df_total[\"p\"].values)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Средняя длина ремарки\n",
    "$$S = \\frac{L}{n},$$\n",
    "где $L$ — количество словоформ (=слов) в ремарке, $n$ — количество ремарок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_direction(directions):\n",
    "    dir_length = []\n",
    "    for direction in directions:\n",
    "        length = len([item for item in mystem.analyze(direction) if \"analysis\" in item])\n",
    "        dir_length.append(length)\n",
    "    return mean(dir_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = []\n",
    "for play in plays:\n",
    "    play_path = directions_path + os.sep + play + \".txt\"\n",
    "    with open(play_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        directions_play = f.readlines()\n",
    "    try:\n",
    "        s = average_direction(directions_play)\n",
    "        print(\"Play: {}, S={:.4f}\".format(play, s))\n",
    "    except:\n",
    "        s = np.nan\n",
    "        print(\"Play: {} failed\")\n",
    "    data_s.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[\"s\"] = data_s\n",
    "df_total[\"s_norm\"] = normalize_metric(data_s)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интенсивность взаимодействия стиха и ремарочной прозы\n",
    "Как часто ремарки появляются внутри стиха?\n",
    "\n",
    "_Адаптация:_ как часто ремарки появляются внутри речи персонажа (`<sp>`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_interruption = re.compile(\"<sp.*?>(\\n.*?)<stage\", flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_i = []\n",
    "for i, play in enumerate(plays):\n",
    "    play_file = corpus_path + os.sep + play + \".xml\"\n",
    "    print(play_file)\n",
    "    with open(play_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        play_text = f.read()\n",
    "        interruption_all = re.findall(reg_interruption, play_text)\n",
    "        dirs_play = data_sp[i]\n",
    "        play_inter = len(interruption_all)/dirs_play\n",
    "        print(\"Play: {}, interruptions: {}, share={:.4f}\".format(play, len(interruption_all), play_inter))\n",
    "        data_i.append(play_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total[\"i\"] = data_i\n",
    "df_total[\"i_norm\"] = normalize_metric(data_i)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коэффициент лексического разнообразия\n",
    "\n",
    "$$V = \\frac{l}{L},$$\n",
    "где $l$ — число лексем, $L$ — число словоформ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmas_wforms_dirs(direction):\n",
    "    lemmas = set()\n",
    "    wforms = set()\n",
    "    analysis_full = mystem.analyze(direction)\n",
    "    for token_analysis in analysis_full:\n",
    "        if \"analysis\" in token_analysis:\n",
    "            try:\n",
    "                lemm = token_analysis[\"analysis\"][0][\"lex\"]\n",
    "                wform = token_analysis[\"text\"]\n",
    "                lemmas.add(lemm)\n",
    "                wforms.add(wform)\n",
    "            except:\n",
    "                pass\n",
    "    return lemmas, wforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lexvar = []\n",
    "for play in plays:\n",
    "    lemmas = set()\n",
    "    wordforms = set()\n",
    "    play_dirs_path = directions_path + os.sep + play + \".txt\"\n",
    "    with open(play_dirs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        play_dirs = [d.strip() for d in f.readlines()]\n",
    "    for p_dir in play_dirs:\n",
    "        dir_lemmas, dir_wordforms = lemmas_wforms_dirs(p_dir)\n",
    "        lemmas.update(dir_lemmas)\n",
    "        wordforms.update(dir_wordforms)\n",
    "    lexvar = len(wordforms)/len(lemmas)\n",
    "    data_lexvar.append(lexvar)\n",
    "    print(\"Play: {}, lexvar={:.4f}\".format(play, lexvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lexvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total[\"l\"] = data_lexvar\n",
    "df_total[\"l_norm\"] = normalize_metric(data_lexvar)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min(df_total[\"p_norm\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: как мы будем считать эмоциональность? sentiment analysis?? вектора??\n",
    "# TODO: почему умножить, а не разделить?\n",
    "def average_classic(p, s, i):\n",
    "    return (p+s+i)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classic = []\n",
    "for i, play in enumerate(plays):\n",
    "    play_p = df_total[\"p_norm\"].values[i] \n",
    "    play_s = df_total[\"s_norm\"].values[i] \n",
    "    play_i = df_total[\"i_norm\"].values[i]\n",
    "    print(\"\\nPlay: {}\".format(play))\n",
    "    classic = average_classic(play_p, play_s, play_i)\n",
    "    print(\"p={:.5f}, s={:.5f}, i={:.5f}\".format(play_p, play_s, play_i))\n",
    "    print(\"classic={:.4f}\".format(classic))\n",
    "    data_classic.append(classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total[\"classic_index\"] = data_classic\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types (according to Sperantov)\n",
    "\n",
    "|**Процент \"классичности\"**|**Тип пьесы**|\n",
    "|:----------------------:|:----------:|\n",
    "|2–10%|достаточно строго следует канону|\n",
    "|10–20%|канон несколько расшатан|\n",
    "|20–45%|значительные отступления от канона|\n",
    "|50–75%|решительно ниспровергают правила классической драматургии|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(classical):\n",
    "    play_type = np.nan\n",
    "    if classical > 0.02 and classical < 0.1:\n",
    "        play_type = \"classic\"\n",
    "    elif classical >= 0.1 and classical < 0.2:\n",
    "        play_type = \"minor_retreat\"\n",
    "    elif classical >= 0.2 and classical < 0.45:\n",
    "        play_type = \"significant_retreat\"\n",
    "    elif classical >= 0.5:\n",
    "        play_type = \"non_classic\"\n",
    "    return play_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total[\"type\"] = df_total[\"classic_index\"].apply(get_type)\n",
    "df_total[\"type\"].astype(\"category\", inplace=True)\n",
    "df_total[\"type\"].cat.set_categories([\"classic\",\"minor_retreat\",\"significal_retreat\",\"non-classic\"],inplace=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распределение по типам // Type distribution\n",
    "\n",
    "Посмотрим на распределение:\n",
    "***\n",
    "Let us take a look at the general distribution (i.e. which types are more common than the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "df_total[\"type\"].value_counts().plot.bar()\n",
    "plt.xticks(rotation=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство — классические или почти классические.\n",
    "***\n",
    "Looks like Russian drama tends to be classic.\n",
    "\n",
    "### Распределение по годам // Distribution by year\n",
    "\n",
    "Теперь посмотрим на распределение по типам в зависимости от года. Я разбила все данные на 10 временных отрезков по 20 лет, чтобы уместить всё на одном графике.\n",
    "\n",
    "Столбцы окрашены в цвет самого \"популярного\" (частотного) типа.\n",
    "***\n",
    "Now we'll look at the distribution by year. To make the visualization clear, I split the whole corpus into 10 groups, each comprising 20 years.\n",
    "\n",
    "The bars you'll see on the graph below are colored according to the \"popular\" (frequent) type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = range(1747, 1767)\n",
    "group2 = range(1767, 1788)\n",
    "group3 = range(1788, 1809)\n",
    "group4 = range(1809, 1830)\n",
    "group5 = range(1830, 1851)\n",
    "group6 = range(1851, 1872)\n",
    "group7 = range(1872, 1893)\n",
    "group8 = range(1893, 1914)\n",
    "group9 = range(1914, 1935)\n",
    "group10 = range(1935, 1956)\n",
    "order = [\"1747–1766\", \"1767–1787\", \"1788–1808\", \"1809–1829\", \"1830–1850\",\n",
    "         \"1851–1872\", \"1872–1893\", \"1893–1914\", \"1914–1934\", \"1935–1955\"]\n",
    "\n",
    "def group_year(year):\n",
    "    y = int(year)\n",
    "    if y in group1:\n",
    "        return \"1747–1766\"\n",
    "    elif y in group2:\n",
    "        return \"1767–1787\"\n",
    "    elif y in group3:\n",
    "        return \"1788–1808\"\n",
    "    elif y in group4:\n",
    "        return \"1809–1829\"\n",
    "    elif y in group5:\n",
    "        return \"1830–1850\"\n",
    "    elif y in group6:\n",
    "        return \"1851–1872\"\n",
    "    elif y in group7:\n",
    "        return \"1872–1893\"\n",
    "    elif y in group8:\n",
    "        return \"1893–1914\"\n",
    "    elif y in group9:\n",
    "        return \"1914–1934\"\n",
    "    elif y in group10:\n",
    "        return \"1935–1955\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total[\"group\"] = df_total[\"year\"].apply(group_year)\n",
    "df_total[\"group\"].astype(\"category\", inplace=True)\n",
    "df_total[\"group\"].cat.set_categories(order, inplace=True)\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = {\n",
    "#     \"classic\": rgb(133, 164, 172),\n",
    "#     \"small_retreat\": rgb(116, 138, 195),\n",
    "#     \"significant_retreat\": rgb(116, 118, 195),\n",
    "#     \"non_classic\": rgb(131, 116, 195)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: подписать количества сверху каждого столбца\n",
    "# TODO: прокрасить столбцы по самому частотному типа (см. cmap выше)\n",
    "# TODISCUSS: не стоит ли сделать их однотонными? разные цвета могут неправильно понять\n",
    "plt.figure(figsize=(16,5))\n",
    "df_total[\"group\"].value_counts().loc[order].plot.bar()\n",
    "plt.title(\"Plays: amount and most common, per period\", fontsize=16)\n",
    "plt.xlabel(\"Group\")\n",
    "plt.ylabel(\"Number of plays\")\n",
    "plt.xticks(rotation=360)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлены подробные распределения по каждому временному отрезку.\n",
    "***\n",
    "More specific data on each group is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1747–1766\"][df_total[\"type\"] == \"classic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df_total,index=[\"group\"],values=[\"type\"],aggfunc=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все пьесы, написанные между 1747 и 1766 годами, классические! На графике однозначно видно, что пьес всего 5, и в таблице выше тоже 5 пьес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1767–1787\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1788–1809\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1809–1829\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1830–1850\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1851–1872\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1872–1893\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1893–1914\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1914–1935\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1935–1955\"][\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[df_total[\"group\"] == \"1935–1955\"][\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.index.shape, df_total[\"group\"].shape, df_total[\"file\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Group**|*classic*|*small_retreat*|*significant_retreat*|*non_classic*|\n",
    "|:-------:|:-------:|:-------------:|:-------------------:|:-----------:|\n",
    "|1767–1787|2        |1              |                     |             |\n",
    "|1788–1808|5        |3              |                     |             |\n",
    "|1809–1829|4        |1              |                     |             |\n",
    "|1830–1850|11       |8              |1                    |             |\n",
    "|1851–1872|3        |12             |5                    |2            |\n",
    "|1872–1893|2        |11             |1                    |1            |\n",
    "|1893–1914|         |6              |6                    |             |\n",
    "|1914–1935|no data  |no data        |no data              |no data      |\n",
    "|1935–1955|         |2              |1                    |             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pivot table\n",
    "# pd.pivot(df_total.index, columns=\"file\", values=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.to_csv(\"./sperantov_WIP.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "# df_total = pd.read_csv(\"./sperantov_WIP.csv\", sep=\";\", encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
