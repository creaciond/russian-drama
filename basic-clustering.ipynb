{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we:\n",
    "\n",
    "* normalize the directions,\n",
    "* represent them as feature vectors (via TF-IDF),\n",
    "* visualise the vectors,\n",
    "* run a simple K-Means clustering algorithm to see where it gets us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start: globals and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions_path = \"..\" + os.sep + \"directions\"\n",
    "csv_path = \".\" + os.sep + \"csv\"\n",
    "corpus_path = \"..\" + os.sep + \"RusDraCor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_df = pd.read_csv(csv_path + os.sep + \"joint_data.csv\", sep=\";\", \n",
    "                 encoding=\"utf-8\", index_col=0)\n",
    "play_df.head()\n",
    "# Островский, Гоголь, Сумароков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_directions_path = directions_path + os.sep + \"all_directions.txt\"\n",
    "with open(all_directions_path, \"r\", encoding=\"utf-8\") as alldirs_file:\n",
    "    alldirs = [line.strip(\"\\n\") for line in alldirs_file.readlines() if line.strip(\"\\r\\n\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many directions we've got now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alldirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and take a look at an example of direction. For instance, at the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldirs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization\n",
    "\n",
    "In order to make clustering easier, I'll normalize all the directions as following:\n",
    "\n",
    "* the words will be turned into their normal form (i.e. _играл -> играть_, _стулья -> стул_, etc.),\n",
    "* stop words (such as interjections) will be removed.\n",
    "\n",
    "The directions are all lowercase already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "stops = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    \n",
    "    lemmas_raw = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    lemmas = [lemma for lemma in lemmas_raw \n",
    "              if lemma not in stops \n",
    "             and lemma not in string.punctuation]\n",
    "    \n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldirs_norm = [normalize(line) for line in alldirs if normalize(line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether this caused a change in amount of directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alldirs) != len(alldirs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alldirs) - len(alldirs_norm)\n",
    "# what's thrown out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also let's take a look at a random direction when it's normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldirs_norm[-1]\n",
    "# list of characters -- compare to get proper names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "\n",
    "I'm vectorizing the directions because it's the easiest way to get numbers out of texts. The algorithm is **TF-IDF**, which is quite common for the NLP tasks and problems. \n",
    "\n",
    "More information:\n",
    "* [scikit-learn page](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) with documentation on functions and parameters,\n",
    "* _I'll probably add an article on that, but I have to find it first! :)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(alldirs_norm)\n",
    "X = tfidf.transform(alldirs_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Now, let's plot what we have to see whether there are any clusters. Unfortunately, we have our TF-IDF results as a sparse matrix, so we'll run a **LSA (latent semantic analysis)** to reduce the number of dimensions down to 2 in order to be able to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# maths behind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(X)\n",
    "X_2d = svd.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yay, machine learning!\n",
    "\n",
    "I'll use **KMeans clustering** algorithm because we have medium amount of directions and 8 clusters (see in [readme](./README.md) — all the classes from TEI classification except for `mixed`). The classes are:\n",
    "\n",
    "1. setting,\n",
    "2. entrance,\n",
    "3. exit,\n",
    "4. business,\n",
    "5. novelistic,\n",
    "6. delivery,\n",
    "7. modifier,\n",
    "8. location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=8)\n",
    "k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_means = k_means.predict(X)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_means, s=50, cmap='viridis')\n",
    "\n",
    "centers = k_means.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "# cluster meanings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
